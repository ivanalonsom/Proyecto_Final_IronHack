{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functions import main_cleaning\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "\n",
    "df_raw = pd.read_csv(\"dielectron.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Iván\\Documentos\\Clase\\Informatica\\Beca IronHack\\Temario\\PROYECTOS\\Proyecto_final_1710\\functions.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"is_same_charge\"] = df[\"Q1\"] == df[\"Q2\"]\n",
      "e:\\Iván\\Documentos\\Clase\\Informatica\\Beca IronHack\\Temario\\PROYECTOS\\Proyecto_final_1710\\functions.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"is_outlier\"] = False\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = main_cleaning(df_raw)\n",
    "df_cleaned.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ml = df_cleaned.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, regularizers\n",
    "# from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "# features_lower = df_ml[[\"Run\", \"pz1\", \"Q1\", \"pz2\", \"Q2\", \"is_outlier\"]]\n",
    "# target_lower = df_ml[\"M\"] \n",
    "\n",
    "# # División del conjunto de datos en entrenamiento y prueba\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features_lower, target_lower, test_size=0.20, random_state=42)\n",
    "\n",
    "# # Normalización de los datos (entre 0 y 1)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Convertir a tensores de TensorFlow\n",
    "# X_train = X_train_scaled.astype('float32')\n",
    "# X_test = X_test_scaled.astype('float32')\n",
    "# y_train = y_train.astype('float32')\n",
    "# y_test = y_test.astype('float32')\n",
    "\n",
    "# # Construir el modelo (regresión)\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(layers.Dropout(0.2))  # Añadir dropout para regularización\n",
    "# model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(layers.Dropout(0.2))  # Añadir dropout para regularización\n",
    "# model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(layers.Dropout(0.2))  # Añadir dropout para regularización\n",
    "# model.add(layers.Dense(1))  # Capa de salida para regresión\n",
    "\n",
    "# # Compilar el modelo\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "# print('Test MAE:', test_mae)\n",
    "\n",
    "# # Predicciones del conjunto de prueba\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calcular métricas adicionales\n",
    "# rmse = root_mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f'Test RMSE: {rmse}')\n",
    "# print(f'Test R²: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, regularizers\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "# features_lower = df_ml[[\"Run\", \"pz1\", \"Q1\", \"pz2\", \"Q2\", \"is_outlier\"]]\n",
    "# target_lower = df_ml[\"M\"] \n",
    "\n",
    "# # División del conjunto de datos en entrenamiento y prueba\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features_lower, target_lower, test_size=0.20, random_state=42)\n",
    "\n",
    "# # Normalización de los datos\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Convertir a tensores de TensorFlow\n",
    "# X_train = X_train_scaled.astype('float32')\n",
    "# X_test = X_test_scaled.astype('float32')\n",
    "# y_train = y_train.astype('float32')\n",
    "# y_test = y_test.astype('float32')\n",
    "\n",
    "# # Construir el modelo (regresión)\n",
    "# model = models.Sequential()\n",
    "# model.add(layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(layers.BatchNormalization())  # Añadir BatchNormalization\n",
    "# model.add(layers.Dropout(0.2))\n",
    "# model.add(layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(layers.BatchNormalization())  # Añadir BatchNormalization\n",
    "# model.add(layers.Dropout(0.2))\n",
    "# model.add(layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "# model.add(layers.BatchNormalization())  # Añadir BatchNormalization\n",
    "# model.add(layers.Dropout(0.2))\n",
    "# model.add(layers.Dense(1))  # Capa de salida para regresión\n",
    "\n",
    "# # Compilar el modelo\n",
    "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# # Early Stopping callback\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# # Entrenar el modelo\n",
    "# history = model.fit(X_train, y_train, epochs=10000, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# # Evaluar el modelo\n",
    "# test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "# print('Test MAE:', test_mae)\n",
    "\n",
    "# # Predicciones del conjunto de prueba\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calcular métricas adicionales\n",
    "# rmse = root_mean_squared_error(y_test, y_pred)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f'Test RMSE: {rmse}')\n",
    "# print(f'Test R²: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\Lib\\site-packages\\scikeras\\wrappers.py:925: UserWarning: ``build_fn`` will be renamed to ``model`` in a future release, at which point use of ``build_fn`` will raise an Error instead.\n",
      "  X, y = self._initialize(X, y)\n",
      "c:\\Anaconda\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.557380 using {'model__neurons3': 64, 'model__neurons2': 128, 'model__neurons1': 256, 'model__learning_rate': 0.001, 'model__l2_rate': 0.001, 'model__dropout_rate': 0.2}\n",
      "Test RMSE: 16.25098991394043\n",
      "Test R²: 0.5856670159261793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Function to create model, required for KerasRegressor\n",
    "def create_model(learning_rate=0.001, neurons1=128, neurons2=64, neurons3=32, dropout_rate=0.2, l2_rate=0.01):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(neurons1, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(neurons2, activation='relu', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(neurons3, activation='relu', kernel_regularizer=regularizers.l2(l2_rate)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dropout(dropout_rate))\n",
    "    model.add(layers.Dense(1))  # Output layer for regression\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Load your data\n",
    "features_lower = df_ml[[\"Run\", \"pz1\", \"Q1\", \"pz2\", \"Q2\", \"is_outlier\"]]\n",
    "target_lower = df_ml[\"M\"]\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_lower, target_lower, test_size=0.20, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to float32\n",
    "X_train = X_train_scaled.astype('float32')\n",
    "X_test = X_test_scaled.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Create a KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters to search\n",
    "param_dist = {\n",
    "    # 'model__batch_size': [32, 64, 128],\n",
    "    # 'model__epochs': [100, 200, 500],\n",
    "    'model__learning_rate': [0.001, 0.01, 0.1],\n",
    "    'model__neurons1': [64, 128, 256],\n",
    "    'model__neurons2': [32, 64, 128],\n",
    "    'model__neurons3': [16, 32, 64],\n",
    "    'model__dropout_rate': [0.2, 0.3, 0.4],\n",
    "    'model__l2_rate': [0.01, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# Create a RandomizedSearchCV object\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "print(\"Best: %f using %s\" % (random_search_result.best_score_, random_search_result.best_params_))\n",
    "\n",
    "# Evaluate the model with the best parameters\n",
    "best_model = random_search_result.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate additional metrics\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Test RMSE: {rmse}')\n",
    "print(f'Test R²: {r2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
